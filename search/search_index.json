{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLAISYS\uff08Let's Learn AI SYStem\uff09","text":"<p>\u672c\u9879\u76ee\u6e90\u81ea \u542f\u5143\u4eba\u5de5\u667a\u80fd\u8bad\u7ec3\u8425 \u00b7 \u5927\u6a21\u578b\u63a8\u7406\u6846\u67b6\u65b9\u5411 \u7684\u8bfe\u7a0b\u4f5c\u4e1a\uff0c\u65e8\u5728\u5e2e\u52a9\u521d\u5b66\u8005\u4ece\u96f6\u6784\u5efa\u5927\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\u3002\u5f53\u524d\u7248\u672c\u5df2\u5b9e\u73b0\u57fa\u7840\u529f\u80fd\uff0c\u5e76\u4f5c\u4e3a\u65b0\u63a8\u7406\u6846\u67b6 zedinfer \u7684\u8bd5\u9a8c\u5e73\u53f0\uff0c\u4e3b\u8981\u7528\u4e8e\uff1a</p> <ul> <li>\u9a8c\u8bc1\u81ea\u5b9a\u4e49\u7b97\u5b50\u5b9e\u73b0\u7684\u6b63\u786e\u6027\uff08\u4e0e Hugging Face \u63a8\u7406\u7ed3\u679c\u5bf9\u9f50\uff09  </li> <li>\u8bc4\u4f30\u81ea\u5b9a\u4e49\u7b97\u5b50\u7684\u6027\u80fd</li> </ul> <p>\ud83d\udccc \u6ce8\u610f\uff1a\u672c\u9879\u76ee\u4e0d\u518d\u65b0\u589e\u529f\u80fd\uff0c\u4ec5\u7528\u4e8e\u7b97\u5b50\u5f00\u53d1\u4e0e\u6027\u80fd\u6d4b\u8bd5\u3002</p>"},{"location":"#_1","title":"\u2705 \u5f53\u524d\u5b9e\u73b0","text":"<ul> <li>\u6a21\u578b\u652f\u6301\uff1aQwen2 \u6a21\u578b\u5728 CPU \u4e0a\u7684\u5b8c\u6574\u63a8\u7406\u6d41\u7a0b\uff08\u542b KV Cache\uff09</li> <li>\u7b97\u5b50\u4f18\u5316\uff1a<ul> <li>\u4f7f\u7528 AVX \u6307\u4ee4\u96c6 \u52a0\u901f\u6838\u5fc3\u8ba1\u7b97</li> <li>\u901a\u8fc7 OpenMP \u5b9e\u73b0\u591a\u7ebf\u7a0b\u5e76\u884c</li> <li>\u9ad8\u6548\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a<ul> <li><code>f16 &lt;=&gt; f32</code>\uff1a\u5229\u7528 F16C \u6307\u4ee4</li> <li><code>bf16 &lt;=&gt; f32</code>\uff1a\u5229\u7528 AVX \u6307\u4ee4</li> </ul> </li> </ul> </li> </ul>"},{"location":"#_2","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_3","title":"\u7f16\u8bd1\u4e0e\u5b89\u88c5","text":"Bash<pre><code># \u7f16\u8bd1 C++ \u540e\u7aef\nxmake\n\n# \u5b89\u88c5\u5171\u4eab\u5e93\nxmake install\n\n# \u5b89\u88c5 Python \u524d\u7aef\u5305\npip install ./python/\n</code></pre>"},{"location":"#_4","title":"\u7b97\u5b50\u6d4b\u8bd5","text":"<p>\u4ee5 <code>add</code> \u7b97\u5b50\u4e3a\u4f8b\u3002</p> Bash<pre><code># \u6b63\u786e\u6027\u6d4b\u8bd5\uff08CPU\uff09\npython test/ops/add.py\n\n# \u6b63\u786e\u6027\u6d4b\u8bd5\uff08NVIDIA GPU\uff09\npython test/ops/add.py --nvidia\n\n# \u6027\u80fd\u5206\u6790\uff08CPU\uff09\npython test/ops/add.py --profile\n\n# \u6027\u80fd\u5206\u6790\uff08NVIDIA GPU\uff09\npython test/ops/add.py --nvidia --profile\n</code></pre>"},{"location":"#_5","title":"\u63a8\u7406\u9a8c\u8bc1","text":"Bash<pre><code>python test/test_infer.py --model /path/to/qwen2/model --test\n</code></pre> <p>\ud83d\udd0d \u542f\u7528 <code>--test</code> \u9009\u9879\u5c06\u5173\u95ed Top-p\u3001Top-k \u548c Temperature \u91c7\u6837\uff0c\u5f3a\u5236 Hugging Face \u4f7f\u7528 ArgMax\uff08\u8d2a\u5a6a\u91c7\u6837\uff09\uff0c\u786e\u4fdd\u4e0e\u672c\u63a8\u7406\u5f15\u64ce\u7684\u91c7\u6837\u7ed3\u679c\u4e25\u683c\u4e00\u81f4\uff0c\u4fbf\u4e8e\u8c03\u8bd5\u4e0e\u9a8c\u8bc1\u3002</p>"},{"location":"#_6","title":"\ud83d\udd1c \u540e\u7eed\u5de5\u4f5c\uff08\u8bd5\u9a8c\u65b9\u5411\uff09","text":"<ol> <li>\u7b97\u5b50\u4f18\u5316\u4e0e\u6269\u5c55<ul> <li>\u8fdb\u4e00\u6b65\u4f18\u5316\u73b0\u6709\u7b97\u5b50\u6027\u80fd</li> <li>\u5b9e\u73b0\u65b0\u7b97\u5b50\uff08\u5982 LayerNorm\u3001Softmax \u7b49\uff09</li> </ul> </li> <li>\u91cf\u5316\u652f\u6301<ul> <li>\u5f00\u53d1 INT4/INT8/FP8 \u91cf\u5316\u7b97\u5b50</li> <li>\u652f\u6301\u7aef\u4fa7\u9ad8\u6548\u63a8\u7406</li> </ul> </li> </ol>"},{"location":"#_7","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90","text":"<ul> <li>\u8bad\u7ec3\u8425\u8bfe\u7a0b\uff1a\u5927\u6a21\u578b\u63a8\u7406\u4e0e\u670d\u52a1\u7cfb\u7edf</li> <li>\u8bfe\u7a0b\u9879\u76ee\u4ed3\u5e93\uff1aInfiniTensor/llaisys</li> </ul>"},{"location":"ops/cpu/","title":"CPU \u7b97\u5b50\u6027\u80fd\u6d4b\u8bc4","text":""},{"location":"ops/cpu/#add","title":"Add","text":"Bash<pre><code>Testing Ops.add on cpu\n   shape (2, 3) dtype &lt;f32&gt;\n        Torch time: 0.00175 ms\n        LLAISYS time: 0.00099 ms\n   shape (2, 3) dtype &lt;f16&gt;\n        Torch time: 0.00172 ms\n        LLAISYS time: 0.00112 ms\n   shape (2, 3) dtype &lt;bf16&gt;\n        Torch time: 0.00180 ms\n        LLAISYS time: 0.00112 ms\n   shape (1, 1536) dtype &lt;f32&gt;\n        Torch time: 0.00196 ms\n        LLAISYS time: 0.00127 ms\n   shape (1, 1536) dtype &lt;f16&gt;\n        Torch time: 0.00227 ms\n        LLAISYS time: 0.00132 ms\n   shape (1, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.00248 ms\n        LLAISYS time: 0.00128 ms\n   shape (1, 4096) dtype &lt;f32&gt;\n        Torch time: 0.00341 ms\n        LLAISYS time: 0.00991 ms\n   shape (1, 4096) dtype &lt;f16&gt;\n        Torch time: 0.00378 ms\n        LLAISYS time: 0.00884 ms\n   shape (1, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.00505 ms\n        LLAISYS time: 0.45665 ms\n   shape (128, 4096) dtype &lt;f32&gt;\n        Torch time: 0.83805 ms\n        LLAISYS time: 0.01307 ms\n   shape (128, 4096) dtype &lt;f16&gt;\n        Torch time: 0.02764 ms\n        LLAISYS time: 0.01084 ms\n   shape (128, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.04782 ms\n        LLAISYS time: 0.01048 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.08329 ms\n        LLAISYS time: 0.08291 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.05232 ms\n        LLAISYS time: 0.02419 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.06497 ms\n        LLAISYS time: 0.02386 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#argmax","title":"Argmax","text":"Bash<pre><code>Testing Ops.argmax on cpu\n   shape (4,) dtype &lt;f32&gt;\n        Torch time: 0.00482 ms\n        LLAISYS time: 0.02570 ms\n   shape (4,) dtype &lt;f16&gt;\n        Torch time: 0.00507 ms\n        LLAISYS time: 0.01987 ms\n   shape (4,) dtype &lt;bf16&gt;\n        Torch time: 0.00480 ms\n        LLAISYS time: 0.01816 ms\n   shape (4096,) dtype &lt;f32&gt;\n        Torch time: 0.01286 ms\n        LLAISYS time: 0.01851 ms\n   shape (4096,) dtype &lt;f16&gt;\n        Torch time: 0.01297 ms\n        LLAISYS time: 0.01930 ms\n   shape (4096,) dtype &lt;bf16&gt;\n        Torch time: 0.01283 ms\n        LLAISYS time: 0.01900 ms\n   shape (151936,) dtype &lt;f32&gt;\n        Torch time: 0.18877 ms\n        LLAISYS time: 0.02389 ms\n   shape (151936,) dtype &lt;f16&gt;\n        Torch time: 0.23977 ms\n        LLAISYS time: 0.03422 ms\n   shape (151936,) dtype &lt;bf16&gt;\n        Torch time: 0.18914 ms\n        LLAISYS time: 0.03777 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#embedding","title":"Embedding","text":"Bash<pre><code>Testing Ops.embedding on cpu\n   idx_shape (1,) embd_shape (2, 3) dtype &lt;f32&gt;\n        Torch time: 0.00948 ms\n        LLAISYS time: 0.00099 ms\n   idx_shape (1,) embd_shape (2, 3) dtype &lt;f16&gt;\n        Torch time: 0.00970 ms\n        LLAISYS time: 0.00099 ms\n   idx_shape (1,) embd_shape (2, 3) dtype &lt;bf16&gt;\n        Torch time: 0.00962 ms\n        LLAISYS time: 0.00100 ms\n   idx_shape (50,) embd_shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.06216 ms\n        LLAISYS time: 0.04720 ms\n   idx_shape (50,) embd_shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.08436 ms\n        LLAISYS time: 0.01256 ms\n   idx_shape (50,) embd_shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.06366 ms\n        LLAISYS time: 0.01257 ms\n   idx_shape (1,) embd_shape (151936, 1536) dtype &lt;f32&gt;\n        Torch time: 0.01073 ms\n        LLAISYS time: 0.00103 ms\n   idx_shape (1,) embd_shape (151936, 1536) dtype &lt;f16&gt;\n        Torch time: 0.01081 ms\n        LLAISYS time: 0.00101 ms\n   idx_shape (1,) embd_shape (151936, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.03640 ms\n        LLAISYS time: 0.00306 ms\n   idx_shape (128,) embd_shape (151936, 1536) dtype &lt;f32&gt;\n        Torch time: 0.07481 ms\n        LLAISYS time: 0.04505 ms\n   idx_shape (128,) embd_shape (151936, 1536) dtype &lt;f16&gt;\n        Torch time: 0.04693 ms\n        LLAISYS time: 0.01327 ms\n   idx_shape (128,) embd_shape (151936, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.07284 ms\n        LLAISYS time: 0.01241 ms\n   idx_shape (1,) embd_shape (151936, 4096) dtype &lt;f32&gt;\n        Torch time: 0.26135 ms\n        LLAISYS time: 0.00165 ms\n   idx_shape (1,) embd_shape (151936, 4096) dtype &lt;f16&gt;\n        Torch time: 0.02105 ms\n        LLAISYS time: 0.00105 ms\n   idx_shape (1,) embd_shape (151936, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.02365 ms\n        LLAISYS time: 0.00104 ms\n   idx_shape (512,) embd_shape (151936, 4096) dtype &lt;f32&gt;\n        Torch time: 8.23859 ms\n        LLAISYS time: 0.80734 ms\n   idx_shape (512,) embd_shape (151936, 4096) dtype &lt;f16&gt;\n        Torch time: 0.18923 ms\n        LLAISYS time: 0.28551 ms\n   idx_shape (512,) embd_shape (151936, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.18550 ms\n        LLAISYS time: 0.27673 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#linear","title":"Linear","text":"Bash<pre><code>Testing Ops.linear on cpu\n   out (2, 3), x (2, 4), w (3, 4), bias True, dtype &lt;f32&gt;\n        Torch time: 0.00648 ms\n        LLAISYS time: 0.02585 ms\n   out (2, 3), x (2, 4), w (3, 4), bias True, dtype &lt;f16&gt;\n        Torch time: 0.01172 ms\n        LLAISYS time: 0.02055 ms\n   out (2, 3), x (2, 4), w (3, 4), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.01145 ms\n        LLAISYS time: 0.02000 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 14.04708 ms\n        LLAISYS time: 15.77965 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 100.69306 ms\n        LLAISYS time: 77.08914 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 60.30512 ms\n        LLAISYS time: 81.89152 ms\n   out (1, 1536), x (1, 1536), w (1536, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.03150 ms\n        LLAISYS time: 0.02055 ms\n   out (1, 1536), x (1, 1536), w (1536, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 0.05552 ms\n        LLAISYS time: 1.97409 ms\n   out (1, 1536), x (1, 1536), w (1536, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 5.52066 ms\n        LLAISYS time: 1.74603 ms\n   out (1, 8960), x (1, 1536), w (8960, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.33603 ms\n        LLAISYS time: 0.32509 ms\n   out (1, 8960), x (1, 1536), w (8960, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 0.24503 ms\n        LLAISYS time: 39.74828 ms\n   out (1, 8960), x (1, 1536), w (8960, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.19489 ms\n        LLAISYS time: 39.73993 ms\n   out (1, 1536), x (1, 8960), w (1536, 8960), bias True, dtype &lt;f32&gt;\n        Torch time: 0.30131 ms\n        LLAISYS time: 0.29183 ms\n   out (1, 1536), x (1, 8960), w (1536, 8960), bias True, dtype &lt;f16&gt;\n        Torch time: 0.25232 ms\n        LLAISYS time: 37.83597 ms\n   out (1, 1536), x (1, 8960), w (1536, 8960), bias True, dtype &lt;bf16&gt;\n        Torch time: 4.24978 ms\n        LLAISYS time: 39.58697 ms\n   out (128, 1536), x (128, 1536), w (1536, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.59083 ms\n        LLAISYS time: 0.78103 ms\n   out (128, 1536), x (128, 1536), w (1536, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 3.91532 ms\n        LLAISYS time: 2.73151 ms\n   out (128, 1536), x (128, 1536), w (1536, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 2.17678 ms\n        LLAISYS time: 2.68255 ms\n   out (128, 8960), x (128, 1536), w (8960, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 9.00348 ms\n        LLAISYS time: 4.38465 ms\n   out (128, 8960), x (128, 1536), w (8960, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 26.22367 ms\n        LLAISYS time: 49.72974 ms\n   out (128, 8960), x (128, 1536), w (8960, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 19.78453 ms\n        LLAISYS time: 45.50789 ms\n   out (128, 1536), x (128, 8960), w (1536, 8960), bias True, dtype &lt;f32&gt;\n        Torch time: 3.00326 ms\n        LLAISYS time: 3.68218 ms\n   out (128, 1536), x (128, 8960), w (1536, 8960), bias True, dtype &lt;f16&gt;\n        Torch time: 20.72079 ms\n        LLAISYS time: 41.66163 ms\n   out (128, 1536), x (128, 8960), w (1536, 8960), bias True, dtype &lt;bf16&gt;\n        Torch time: 12.54653 ms\n        LLAISYS time: 39.94967 ms\n   out (1, 4096), x (1, 4096), w (4096, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 0.43818 ms\n        LLAISYS time: 0.44445 ms\n   out (1, 4096), x (1, 4096), w (4096, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.21999 ms\n        LLAISYS time: 44.09353 ms\n   out (1, 4096), x (1, 4096), w (4096, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.22669 ms\n        LLAISYS time: 44.41967 ms\n   out (1, 12288), x (1, 4096), w (12288, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 2.53126 ms\n        LLAISYS time: 2.39524 ms\n   out (1, 12288), x (1, 4096), w (12288, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.96195 ms\n        LLAISYS time: 129.10251 ms\n   out (1, 12288), x (1, 4096), w (12288, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 1.10616 ms\n        LLAISYS time: 128.09046 ms\n   out (1, 4096), x (1, 12288), w (4096, 12288), bias True, dtype &lt;f32&gt;\n        Torch time: 3.16014 ms\n        LLAISYS time: 3.27317 ms\n   out (1, 4096), x (1, 12288), w (4096, 12288), bias True, dtype &lt;f16&gt;\n        Torch time: 1.24868 ms\n        LLAISYS time: 132.19068 ms\n   out (1, 4096), x (1, 12288), w (4096, 12288), bias True, dtype &lt;bf16&gt;\n        Torch time: 2.76618 ms\n        LLAISYS time: 132.62828 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 16.28651 ms\n        LLAISYS time: 19.26332 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 110.08841 ms\n        LLAISYS time: 81.61504 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 62.56967 ms\n        LLAISYS time: 77.58017 ms\n   out (512, 12288), x (512, 4096), w (12288, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 68.13789 ms\n        LLAISYS time: 45.01108 ms\n   out (512, 12288), x (512, 4096), w (12288, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 293.34185 ms\n        LLAISYS time: 177.92957 ms\n   out (512, 12288), x (512, 4096), w (12288, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 163.96168 ms\n        LLAISYS time: 189.57881 ms\n   out (512, 4096), x (512, 12288), w (4096, 12288), bias True, dtype &lt;f32&gt;\n        Torch time: 42.86984 ms\n        LLAISYS time: 51.26665 ms\n   out (512, 4096), x (512, 12288), w (4096, 12288), bias True, dtype &lt;f16&gt;\n        Torch time: 324.15250 ms\n        LLAISYS time: 177.71885 ms\n   out (512, 4096), x (512, 12288), w (4096, 12288), bias True, dtype &lt;bf16&gt;\n        Torch time: 153.70608 ms\n        LLAISYS time: 179.06548 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#rms-norm","title":"RMS Norm","text":"Bash<pre><code>Testing Ops.rms_norm on cpu\n   shape (1, 4) dtype &lt;f32&gt;\n        Torch time: 0.09181 ms\n        LLAISYS time: 1.83623 ms\n   shape (1, 4) dtype &lt;f16&gt;\n        Torch time: 0.03762 ms\n        LLAISYS time: 0.00984 ms\n   shape (1, 4) dtype &lt;bf16&gt;\n        Torch time: 0.04192 ms\n        LLAISYS time: 0.00864 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.22941 ms\n        LLAISYS time: 0.06763 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.73621 ms\n        LLAISYS time: 0.20431 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.26237 ms\n        LLAISYS time: 0.19928 ms\n   shape (1, 1536) dtype &lt;f32&gt;\n        Torch time: 0.03239 ms\n        LLAISYS time: 0.00689 ms\n   shape (1, 1536) dtype &lt;f16&gt;\n        Torch time: 0.04396 ms\n        LLAISYS time: 0.00791 ms\n   shape (1, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.04061 ms\n        LLAISYS time: 0.00765 ms\n   shape (1, 4096) dtype &lt;f32&gt;\n        Torch time: 0.03402 ms\n        LLAISYS time: 0.00785 ms\n   shape (1, 4096) dtype &lt;f16&gt;\n        Torch time: 0.05662 ms\n        LLAISYS time: 0.01062 ms\n   shape (1, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.04314 ms\n        LLAISYS time: 0.01024 ms\n   shape (128, 4096) dtype &lt;f32&gt;\n        Torch time: 0.09694 ms\n        LLAISYS time: 0.02086 ms\n   shape (128, 4096) dtype &lt;f16&gt;\n        Torch time: 0.30821 ms\n        LLAISYS time: 0.05599 ms\n   shape (128, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.18093 ms\n        LLAISYS time: 0.05431 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.23789 ms\n        LLAISYS time: 0.06826 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.72731 ms\n        LLAISYS time: 0.20435 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.26178 ms\n        LLAISYS time: 0.19929 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#rope","title":"RoPE","text":"Bash<pre><code>Testing Ops.rope on cpu\n   shape (2, 1, 4) range (0, 2) dtype &lt;f32&gt;\n        Torch time: 0.09558 ms\n        LLAISYS time: 0.00907 ms\n   shape (2, 1, 4) range (0, 2) dtype &lt;f16&gt;\n        Torch time: 0.11036 ms\n        LLAISYS time: 0.00876 ms\n   shape (2, 1, 4) range (0, 2) dtype &lt;bf16&gt;\n        Torch time: 0.10958 ms\n        LLAISYS time: 0.00849 ms\n   shape (512, 4, 4096) range (512, 1024) dtype &lt;f32&gt;\n        Torch time: 16.56124 ms\n        LLAISYS time: 6.39340 ms\n   shape (512, 4, 4096) range (512, 1024) dtype &lt;f16&gt;\n        Torch time: 25.94199 ms\n        LLAISYS time: 8.13366 ms\n   shape (512, 4, 4096) range (512, 1024) dtype &lt;bf16&gt;\n        Torch time: 24.68366 ms\n        LLAISYS time: 8.53675 ms\n   shape (1, 12, 128) range (128, 129) dtype &lt;f32&gt;\n        Torch time: 0.10193 ms\n        LLAISYS time: 0.01230 ms\n   shape (1, 12, 128) range (128, 129) dtype &lt;f16&gt;\n        Torch time: 0.11763 ms\n        LLAISYS time: 0.01304 ms\n   shape (1, 12, 128) range (128, 129) dtype &lt;bf16&gt;\n        Torch time: 0.11603 ms\n        LLAISYS time: 0.01322 ms\n   shape (1, 2, 128) range (128, 129) dtype &lt;f32&gt;\n        Torch time: 0.10122 ms\n        LLAISYS time: 0.01124 ms\n   shape (1, 2, 128) range (128, 129) dtype &lt;f16&gt;\n        Torch time: 0.11446 ms\n        LLAISYS time: 0.01176 ms\n   shape (1, 2, 128) range (128, 129) dtype &lt;bf16&gt;\n        Torch time: 0.11372 ms\n        LLAISYS time: 0.01154 ms\n   shape (128, 12, 128) range (0, 128) dtype &lt;f32&gt;\n        Torch time: 0.26333 ms\n        LLAISYS time: 0.14725 ms\n   shape (128, 12, 128) range (0, 128) dtype &lt;f16&gt;\n        Torch time: 0.36035 ms\n        LLAISYS time: 0.21987 ms\n   shape (128, 12, 128) range (0, 128) dtype &lt;bf16&gt;\n        Torch time: 0.35083 ms\n        LLAISYS time: 0.18147 ms\n   shape (128, 2, 128) range (0, 128) dtype &lt;f32&gt;\n        Torch time: 0.15896 ms\n        LLAISYS time: 0.03249 ms\n   shape (128, 2, 128) range (0, 128) dtype &lt;f16&gt;\n        Torch time: 0.19934 ms\n        LLAISYS time: 0.03969 ms\n   shape (128, 2, 128) range (0, 128) dtype &lt;bf16&gt;\n        Torch time: 0.20182 ms\n        LLAISYS time: 0.03874 ms\n   shape (512, 32, 128) range (0, 512) dtype &lt;f32&gt;\n        Torch time: 0.47251 ms\n        LLAISYS time: 1.53435 ms\n   shape (512, 32, 128) range (0, 512) dtype &lt;f16&gt;\n        Torch time: 0.54411 ms\n        LLAISYS time: 1.98834 ms\n   shape (512, 32, 128) range (0, 512) dtype &lt;bf16&gt;\n        Torch time: 0.60431 ms\n        LLAISYS time: 1.97814 ms\n   shape (512, 8, 128) range (0, 512) dtype &lt;f32&gt;\n        Torch time: 0.25050 ms\n        LLAISYS time: 0.38841 ms\n   shape (512, 8, 128) range (0, 512) dtype &lt;f16&gt;\n        Torch time: 0.31573 ms\n        LLAISYS time: 0.50686 ms\n   shape (512, 8, 128) range (0, 512) dtype &lt;bf16&gt;\n        Torch time: 0.31922 ms\n        LLAISYS time: 0.50523 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#self-attention","title":"Self-Attention","text":"Bash<pre><code>Testing Ops.self_attention on cpu\n   qlen=2 kvlen=2 nh=1 nkvh=1 hd=4 dtype &lt;f32&gt;\n        Torch time: 0.12779 ms\n        LLAISYS time: 0.00886 ms\n   qlen=2 kvlen=2 nh=1 nkvh=1 hd=4 dtype &lt;f16&gt;\n        Torch time: 0.12759 ms\n        LLAISYS time: 0.00850 ms\n   qlen=2 kvlen=2 nh=1 nkvh=1 hd=4 dtype &lt;bf16&gt;\n        Torch time: 0.12764 ms\n        LLAISYS time: 0.00849 ms\n   qlen=5 kvlen=11 nh=4 nkvh=2 hd=8 dtype &lt;f32&gt;\n        Torch time: 0.15392 ms\n        LLAISYS time: 0.01010 ms\n   qlen=5 kvlen=11 nh=4 nkvh=2 hd=8 dtype &lt;f16&gt;\n        Torch time: 0.20284 ms\n        LLAISYS time: 0.01088 ms\n   qlen=5 kvlen=11 nh=4 nkvh=2 hd=8 dtype &lt;bf16&gt;\n        Torch time: 0.19987 ms\n        LLAISYS time: 0.01021 ms\n   qlen=128 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.46535 ms\n        LLAISYS time: 1.31248 ms\n   qlen=128 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f16&gt;\n        Torch time: 37.87175 ms\n        LLAISYS time: 7.05592 ms\n   qlen=128 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.85768 ms\n        LLAISYS time: 12.71847 ms\n   qlen=1 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.22697 ms\n        LLAISYS time: 0.03806 ms\n   qlen=1 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f16&gt;\n        Torch time: 0.81133 ms\n        LLAISYS time: 0.18547 ms\n   qlen=1 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.27036 ms\n        LLAISYS time: 0.15310 ms\n   qlen=512 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f32&gt;\n        Torch time: 30.89260 ms\n        LLAISYS time: 107.49103 ms\n   qlen=512 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f16&gt;\n        Torch time: 1271.51130 ms\n        LLAISYS time: 310.16958 ms\n   qlen=512 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;bf16&gt;\n        Torch time: 22.05624 ms\n        LLAISYS time: 309.82454 ms\n   qlen=1 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.63585 ms\n        LLAISYS time: 0.68920 ms\n   qlen=1 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f16&gt;\n        Torch time: 4.14387 ms\n        LLAISYS time: 1.19079 ms\n   qlen=1 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.44024 ms\n        LLAISYS time: 1.14253 ms\nTest passed!\n</code></pre>"},{"location":"ops/cpu/#swiglu","title":"SwiGLU","text":"Bash<pre><code>esting Ops.swiglu on cpu\n   shape (2, 3) dtype &lt;f32&gt;\n        Torch time: 0.02364 ms\n        LLAISYS time: 0.01004 ms\n   shape (2, 3) dtype &lt;f16&gt;\n        Torch time: 0.02983 ms\n        LLAISYS time: 0.00875 ms\n   shape (2, 3) dtype &lt;bf16&gt;\n        Torch time: 0.03146 ms\n        LLAISYS time: 0.00900 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 2.36821 ms\n        LLAISYS time: 0.92981 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.39675 ms\n        LLAISYS time: 1.60204 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.41408 ms\n        LLAISYS time: 1.42567 ms\n   shape (128, 8960) dtype &lt;f32&gt;\n        Torch time: 0.18528 ms\n        LLAISYS time: 0.49367 ms\n   shape (128, 8960) dtype &lt;f16&gt;\n        Torch time: 0.25142 ms\n        LLAISYS time: 0.85083 ms\n   shape (128, 8960) dtype &lt;bf16&gt;\n        Torch time: 0.30128 ms\n        LLAISYS time: 0.78367 ms\n   shape (1, 8960) dtype &lt;f32&gt;\n        Torch time: 0.03794 ms\n        LLAISYS time: 0.00957 ms\n   shape (1, 8960) dtype &lt;f16&gt;\n        Torch time: 0.04749 ms\n        LLAISYS time: 0.01219 ms\n   shape (1, 8960) dtype &lt;bf16&gt;\n        Torch time: 0.05170 ms\n        LLAISYS time: 0.01164 ms\n   shape (512, 12288) dtype &lt;f32&gt;\n        Torch time: 15.23693 ms\n        LLAISYS time: 2.75612 ms\n   shape (512, 12288) dtype &lt;f16&gt;\n        Torch time: 16.07239 ms\n        LLAISYS time: 4.70460 ms\n   shape (512, 12288) dtype &lt;bf16&gt;\n        Torch time: 19.06272 ms\n        LLAISYS time: 4.35105 ms\n   shape (1, 12288) dtype &lt;f32&gt;\n        Torch time: 0.04549 ms\n        LLAISYS time: 0.01086 ms\n   shape (1, 12288) dtype &lt;f16&gt;\n        Torch time: 0.05562 ms\n        LLAISYS time: 0.01450 ms\n   shape (1, 12288) dtype &lt;bf16&gt;\n        Torch time: 0.06165 ms\n        LLAISYS time: 0.01384 ms\nTest passed!\n</code></pre>"},{"location":"ops/list/","title":"\u7b97\u5b50\u5217\u8868","text":"<p>\u76ee\u524d\u4ec5\u652f\u6301 <code>Float32</code>\u3001<code>Float16</code> \u548c <code>BFloat16</code> \u6570\u636e\u7c7b\u578b\u3002</p>"},{"location":"ops/list/#add","title":"Add\uff08\u52a0\u6cd5\uff09","text":"C++<pre><code>void add(tensor_t c, tensor_t a, tensor_t b);\n</code></pre> <p>\u5bf9\u5f20\u91cf <code>a</code> \u548c <code>b</code> \u6267\u884c\u9010\u5143\u7d20\u52a0\u6cd5\uff0c\u7ed3\u679c\u5b58\u5165\u5f20\u91cf <code>c</code>\u3002\u6240\u6709\u8f93\u5165\u4e0e\u8f93\u51fa\u5f20\u91cf\u5fc5\u987b\u5177\u6709\u76f8\u540c\u5f62\u72b6\u4e14\u4e3a\u8fde\u7eed\u5185\u5b58\u5e03\u5c40\u3002</p>"},{"location":"ops/list/#argmax","title":"Argmax\uff08\u53d6\u6700\u5927\u503c\u7d22\u5f15\uff09","text":"C++<pre><code>void argmax(tensor_t max_idx, tensor_t max_val, tensor_t vals);\n</code></pre> <p>\u5728 1D \u8f93\u5165\u5f20\u91cf <code>vals</code> \u4e2d\u67e5\u627e\u6700\u5927\u503c\u53ca\u5176\u5bf9\u5e94\u7d22\u5f15\u3002\u6700\u5927\u503c\u5199\u5165 <code>max_val</code>\uff0c\u5176\u7d22\u5f15\u5199\u5165 <code>max_idx</code>\u3002<code>max_val</code> \u4e0e <code>max_idx</code> \u5747\u4e3a\u5305\u542b\u5355\u4e2a\u5143\u7d20\u7684 1D \u5f20\u91cf\u3002\u8f93\u5165 <code>vals</code> \u5fc5\u987b\u662f 1D \u4e14\u8fde\u7eed\u7684\u5f20\u91cf\u3002</p>"},{"location":"ops/list/#embedding","title":"Embedding\uff08\u5d4c\u5165\u67e5\u627e\uff09","text":"C++<pre><code>void embedding(tensor_t out, tensor_t index, tensor_t weight);\n</code></pre> <p>\u6839\u636e 1D \u7684 <code>index</code> \u5411\u91cf\uff08\u6570\u636e\u7c7b\u578b\u4e3a <code>int64</code>\uff09\uff0c\u4ece 2D \u7684 <code>weight</code> \u77e9\u9635\u4e2d\u6309\u884c\u7d22\u5f15\u53d6\u51fa\u5bf9\u5e94\u884c\uff0c\u7ed3\u679c\u5199\u5165 2D \u8f93\u51fa\u5f20\u91cf <code>out</code>\u3002<code>weight</code> \u5fc5\u987b\u662f 2D \u8fde\u7eed\u5f20\u91cf\uff0c<code>index</code> \u5fc5\u987b\u662f 1D \u7684 <code>int64</code> \u5f20\u91cf\u3002</p>"},{"location":"ops/list/#linear","title":"Linear\uff08\u7ebf\u6027\u53d8\u6362\uff09","text":"C++<pre><code>void linear(tensor_t out, tensor_t in, tensor_t weight, tensor_t bias);\n</code></pre> \\[ Y = XW^T + b \\] <p>\u8ba1\u7b97\u7ebf\u6027\u53d8\u6362 \\(Y = XW^T + b\\)\uff0c\u5176\u4e2d\uff1a</p> <ul> <li><code>in</code> \u5373 \\(X\\)\uff0c\u4e3a 2D \u8fde\u7eed\u8f93\u5165\u5f20\u91cf\uff1b</li> <li><code>weight</code> \u5373 \\(W\\)\uff0c\u4e3a 2D \u8fde\u7eed\u6743\u91cd\u5f20\u91cf\uff08\u65e0\u9700\u9884\u5148\u8f6c\u7f6e\uff09\uff1b</li> <li><code>bias</code> \u5373 \\(b\\)\uff0c\u4e3a\u53ef\u9009\u7684 1D \u504f\u7f6e\u5f20\u91cf\uff08\u82e5\u672a\u63d0\u4f9b\uff0c\u5219\u4e0d\u52a0\u504f\u7f6e\uff09\uff1b</li> <li><code>out</code> \u5373 \\(Y\\)\uff0c\u4e3a 2D \u8fde\u7eed\u8f93\u51fa\u5f20\u91cf\u3002</li> </ul>"},{"location":"ops/list/#rms-normalization","title":"RMS Normalization\uff08\u5747\u65b9\u6839\u5f52\u4e00\u5316\uff09","text":"C++<pre><code>void rms_norm(tensor_t out, tensor_t in, tensor_t weight, float eps);\n</code></pre> <p>\u5bf9\u8f93\u5165\u5f20\u91cf <code>in</code> \u7684\u6bcf\u4e00\u884c\u6cbf\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c RMS \u5f52\u4e00\u5316\uff0c\u516c\u5f0f\u4e3a\uff1a</p> \\[ Y_i = \\frac{W \\odot  X_i}{\\sqrt{\\frac{1}{d} \\sum_{j=1}^d X_{i,j}^2 + \\varepsilon}} \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li><code>in</code> \u5373 \\(X\\)\uff0c\u4e3a 2D \u8fde\u7eed\u8f93\u5165\u5f20\u91cf\uff1b</li> <li><code>weight</code> \u5373 \\(W\\)\uff0c\u4e3a\u957f\u5ea6\u7b49\u4e8e\u8f93\u5165\u884c\u5bbd \\(d\\) \u7684 1D \u6743\u91cd\u5f20\u91cf\uff1b</li> <li><code>eps</code> \u5373 \\(\\varepsilon\\)\uff0c\u4e3a\u9632\u6b62\u9664\u96f6\u7684\u5c0f\u5e38\u6570\uff1b</li> <li><code>out</code> \u5373 \\(Y\\)\uff0c\u4e3a 2D \u8fde\u7eed\u8f93\u51fa\u5f20\u91cf\u3002</li> </ul>"},{"location":"ops/list/#rope","title":"RoPE\uff08\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff09","text":"<p>C++<pre><code>void rope(tensor_t out, tensor_t in, tensor_t pos_ids, float theta);\n</code></pre> \u4e3a\u8f93\u5165\u5f20\u91cf<code>in</code>\u7684\u6bcf\u4e2a\u5411\u91cf\uff08\u8fd9\u4e9b\u5411\u91cf\u4e0e pos_ids \u4e2d\u7684\u4f4d\u7f6e id \u76f8\u5bf9\u5e94\uff09\u8ba1\u7b97\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <p>\u8bbe \\(\\mathbf{x}_i = [\\mathbf{a}_i, \\mathbf{b}_i] \\in \\mathbb{R}^d\\) \u4e3a\u8f93\u5165\u5411\u91cf\uff0c \\(\\mathbf{y}_i = [\\mathbf{a}'_i, \\mathbf{b}'_i] \\in \\mathbb{R}^d\\) \u4e3a\u7d22\u5f15 \\(i\\) \u5904\u7684\u8f93\u51fa\u5411\u91cf\uff0c\u5176\u4e2d \\(\\mathbf{a}_i, \\mathbf{b}_i,\\mathbf{a}'_i, \\mathbf{b}'_i \\in \\mathbb{R}^{d/2}\\) \u3002</p> <p>\u8bbe \\(\\theta\\) \u4e3a\u56fa\u5b9a\u57fa\u6570\uff08\u4f8b\u5982 \\(\\theta = 10000\\)\uff09\uff0c \\(j = 0, 1, \\ldots, d/2 - 1\\)\u3002</p> <p>\u8bbe \\(p_i \\in \\mathbb{N}\\) \u662f\u8f93\u5165\u7d22\u5f15 \\(i\\) \u5904token\u7684\u4f4d\u7f6eid\u3002</p> <p>\u90a3\u4e48RoPE\u7684\u89d2\u5ea6\u4e3a \\(\\phi_{i,j} = \\frac{p_i}{\\theta^{2j/d}}\\)</p> <p>\u8f93\u51fa\u5411\u91cf \\(\\mathbf{y}_i = [\\mathbf{a}'_i, \\mathbf{b}'_i]\\) \u8ba1\u7b97\u5982\u4e0b\uff1a</p> \\[a_{i,j}' = a_{i,j} \\cos(\\phi_{i,j}) - b_{i,j} \\sin(\\phi_{i,j})\\] \\[b_{i,j}' = b_{i,j} \\cos(\\phi_{i,j}) + a_{i,j} \\sin(\\phi_{i,j})\\] <ul> <li><code>out</code>\uff1a\u7f16\u7801\u540e\u7684\u67e5\u8be2\uff08Q\uff09\u6216\u952e\uff08K\uff09\u5f20\u91cf\u3002\u5f62\u72b6\u5e94\u8be5\u662f <code>[seqlen, nhead, d]</code> \u6216 <code>[seqlen, nkvhead, d]</code>\u3002\u6682\u65f6\u53ef\u4ee5\u5047\u8bbe\u5f20\u91cf\u662f\u8fde\u7eed\u7684\u3002</li> <li><code>in</code>\uff1a\u539f\u59cb\u67e5\u8be2\uff08Q\uff09\u6216\u952e\uff08K\uff09\u5f20\u91cf\u3002\u5f62\u72b6\u5e94\u8be5\u662f <code>[seqlen, nhead, d]</code> \u6216 <code>[seqlen, nkvhead, d]</code>\u3002\u6682\u65f6\u53ef\u4ee5\u5047\u8bbe\u5f20\u91cf\u662f\u8fde\u7eed\u7684\u3002</li> <li><code>pos_ids</code>\uff1a\u8f93\u5165\u5e8f\u5217\u4e2d\u6bcf\u4e2atoken\u7684\u4f4d\u7f6eid\uff08\u6574\u4e2a\u4e0a\u4e0b\u6587\u4e2d\u7684\u7d22\u5f15\uff09\u3002\u5f62\u72b6\u5e94\u8be5\u662f <code>[seqlen,]</code>\uff0cdtype\u5e94\u8be5\u662f<code>int64</code>\u3002</li> <li><code>theta</code>\uff1a\u9891\u7387\u5411\u91cf\u7684\u57fa\u503c\uff08\u5982 10000\uff09\u3002</li> </ul>"},{"location":"ops/list/#self-attention","title":"Self-Attention\uff08\u81ea\u6ce8\u610f\u529b\uff09","text":"<p>C++<pre><code>void self_attention(tensor_t attn_val, tensor_t q, tensor_t k, tensor_t v, float scale);\n</code></pre> \u4e3a\u67e5\u8be2\u5f20\u91cf<code>q</code>\u3001\u952e\u5f20\u91cf<code>k</code>\u548c\u503c\u5f20\u91cf<code>v</code>\u8ba1\u7b97\u5e26\u56e0\u679c\u63a9\u7801\u7684\u81ea\u6ce8\u610f\u529b\u3002</p> \\[ A = Q K^\\top * scale \\\\ \\] \\[ Y = \\mathrm{causalsoftmax}(A) \\cdot V \\\\ \\] <ul> <li><code>attn_val</code>\uff1a\u7ed3\u679c\u6ce8\u610f\u529b\u503c\u5f20\u91cf\u3002\u5f62\u72b6\u5e94\u8be5\u662f<code>[seqlen, nhead, dv]</code>\u3002\u6682\u65f6\u53ef\u4ee5\u5047\u8bbe\u5f20\u91cf\u662f\u8fde\u7eed\u7684\u3002</li> <li><code>q</code>\uff1a\u67e5\u8be2\u5f20\u91cf\u3002\u5f62\u72b6\u5e94\u8be5\u662f <code>[seqlen, nhead, d]</code>\u3002\u6682\u65f6\u53ef\u4ee5\u5047\u8bbe\u5f20\u91cf\u662f\u8fde\u7eed\u7684\u3002</li> <li><code>k</code>\uff1a\u952e\u5f20\u91cf\u3002\u5f62\u72b6\u5e94\u8be5\u662f <code>[total_len, nkvhead, d]</code>\u3002\u6682\u65f6\u53ef\u4ee5\u5047\u8bbe\u5f20\u91cf\u662f\u8fde\u7eed\u7684\u3002</li> <li><code>v</code>\uff1a\u503c\u5f20\u91cf\u3002\u5f62\u72b6\u5e94\u8be5\u662f <code>[total_len, nkvhead, dv]</code>\u3002\u6682\u65f6\u53ef\u4ee5\u5047\u8bbe\u5f20\u91cf\u662f\u8fde\u7eed\u7684\u3002</li> <li><code>scale</code>\uff1a\u7f29\u653e\u56e0\u5b50\u3002\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u53d6\u503c\u4e3a \\(\\frac{1}{\\sqrt{d}}\\) \u3002</li> </ul> <p><code>total_len</code> = <code>past_len</code> + <code>seq_len</code>\uff0c\u8ba1\u7b97\u5f53\u524d\u6279\u6b21<code>seq_len</code>\u4e2atoken\u7684attention\u65f6\uff0c\u9700\u8981\u6ce8\u610f\u5230\u524d\u9762\u7684<code>past_len</code>\u4e2atoken\u4ee5\u53ca\u5f53\u524d\u6279\u6b21\u7684<code>1..seq_len</code>\u4e2atoken\u7684\u952e\uff08K\uff09,\u6b64\u5904\u9700\u8981\u6ce8\u610f kvcache \u7684\u62fc\u63a5\u3002</p>"},{"location":"ops/list/#swigluswish-gated-linear-unit","title":"SwiGLU\uff08Swish-Gated Linear Unit\uff09","text":"C++<pre><code>void swiglu(tensor_t out, tensor_t gate, tensor_t up);\n</code></pre> <p>\u8ba1\u7b97 SwiGLU \u6fc0\u6d3b\u51fd\u6570\uff1a</p> \\[ out_{i} = up_{i} \\odot \\frac { gate_{i}}{1 + e^{-gate_{i}}} \\] <p>\\(e^{\u2212gate_i}\\) \u8868\u793a\u5bf9 \\(gate_i\\) \u5411\u91cf\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5355\u72ec\u8fdb\u884c\u6307\u6570\u8fd0\u7b97\u3002</p> <p>\u5176\u4e2d <code>gate</code>\u3001<code>up</code> \u548c <code>out</code> \u5747\u4e3a\u5f62\u72b6\u76f8\u540c\u7684 2D \u8fde\u7eed\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a <code>[seqlen, intermediate_size]</code>\u3002</p>"},{"location":"ops/list/#rearrange","title":"Rearrange\uff08\u91cd\u6392\uff09","text":"C++<pre><code>void rearrange(tensor_t out, tensor_t in);\n</code></pre> <p>\u5c06\u6570\u636e\u4ece\u8f93\u5165\u5f20\u91cf <code>in</code> \u590d\u5236\u5230\u8f93\u51fa\u5f20\u91cf <code>out</code>\uff0c\u4e24\u8005\u5177\u6709\u76f8\u540c\u903b\u8f91\u5f62\u72b6\u4f46\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u5185\u5b58\u6b65\u957f\uff08strides\uff09\u3002\u8be5\u7b97\u5b50\u53ef\u7528\u4e8e\u5b9e\u73b0\u5f20\u91cf\u7684 <code>contiguous()</code> \u529f\u80fd\uff0c\u786e\u4fdd\u8f93\u51fa\u4e3a\u8fde\u7eed\u5185\u5b58\u5e03\u5c40\u3002</p>"},{"location":"ops/nvidia/","title":"NVIDIA \u7b97\u5b50\u6027\u80fd\u6d4b\u8bc4","text":""},{"location":"ops/nvidia/#add","title":"Add","text":"Bash<pre><code>Testing Ops.add on nvidia\n   shape (2, 3) dtype &lt;f32&gt;\n        Torch time: 0.00576 ms\n        LLAISYS time: 0.00393 ms\n   shape (2, 3) dtype &lt;f16&gt;\n        Torch time: 0.00586 ms\n        LLAISYS time: 0.00387 ms\n   shape (2, 3) dtype &lt;bf16&gt;\n        Torch time: 0.00571 ms\n        LLAISYS time: 0.00388 ms\n   shape (1, 1536) dtype &lt;f32&gt;\n        Torch time: 0.00564 ms\n        LLAISYS time: 0.00384 ms\n   shape (1, 1536) dtype &lt;f16&gt;\n        Torch time: 0.00559 ms\n        LLAISYS time: 0.00396 ms\n   shape (1, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.00575 ms\n        LLAISYS time: 0.00395 ms\n   shape (1, 4096) dtype &lt;f32&gt;\n        Torch time: 0.00560 ms\n        LLAISYS time: 0.00395 ms\n   shape (1, 4096) dtype &lt;f16&gt;\n        Torch time: 0.00576 ms\n        LLAISYS time: 0.00389 ms\n   shape (1, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.00574 ms\n        LLAISYS time: 0.00390 ms\n   shape (128, 4096) dtype &lt;f32&gt;\n        Torch time: 0.00561 ms\n        LLAISYS time: 0.00395 ms\n   shape (128, 4096) dtype &lt;f16&gt;\n        Torch time: 0.00560 ms\n        LLAISYS time: 0.00385 ms\n   shape (128, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.00554 ms\n        LLAISYS time: 0.00391 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.00736 ms\n        LLAISYS time: 0.00648 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.00583 ms\n        LLAISYS time: 0.00595 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.00572 ms\n        LLAISYS time: 0.00595 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#argmax","title":"Argmax","text":"Bash<pre><code>Testing Ops.argmax on nvidia\n   shape (4,) dtype &lt;f32&gt;\n        Torch time: 0.00891 ms\n        LLAISYS time: 0.01019 ms\n   shape (4,) dtype &lt;f16&gt;\n        Torch time: 0.00886 ms\n        LLAISYS time: 0.01017 ms\n   shape (4,) dtype &lt;bf16&gt;\n        Torch time: 0.00867 ms\n        LLAISYS time: 0.01016 ms\n   shape (4096,) dtype &lt;f32&gt;\n        Torch time: 0.00922 ms\n        LLAISYS time: 0.01025 ms\n   shape (4096,) dtype &lt;f16&gt;\n        Torch time: 0.00871 ms\n        LLAISYS time: 0.01042 ms\n   shape (4096,) dtype &lt;bf16&gt;\n        Torch time: 0.00888 ms\n        LLAISYS time: 0.01049 ms\n   shape (151936,) dtype &lt;f32&gt;\n        Torch time: 0.01246 ms\n        LLAISYS time: 0.01090 ms\n   shape (151936,) dtype &lt;f16&gt;\n        Torch time: 0.01237 ms\n        LLAISYS time: 0.01073 ms\n   shape (151936,) dtype &lt;bf16&gt;\n        Torch time: 0.01230 ms\n        LLAISYS time: 0.01087 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#embedding","title":"Embedding","text":"Bash<pre><code>Testing Ops.embedding on nvidia\n   idx_shape (1,) embd_shape (2, 3) dtype &lt;f32&gt;\n        Torch time: 0.03082 ms\n        LLAISYS time: 0.00601 ms\n   idx_shape (1,) embd_shape (2, 3) dtype &lt;f16&gt;\n        Torch time: 0.03075 ms\n        LLAISYS time: 0.00591 ms\n   idx_shape (1,) embd_shape (2, 3) dtype &lt;bf16&gt;\n        Torch time: 0.02378 ms\n        LLAISYS time: 0.00411 ms\n   idx_shape (50,) embd_shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.02405 ms\n        LLAISYS time: 0.00402 ms\n   idx_shape (50,) embd_shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.02422 ms\n        LLAISYS time: 0.00405 ms\n   idx_shape (50,) embd_shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.02411 ms\n        LLAISYS time: 0.00398 ms\n   idx_shape (1,) embd_shape (151936, 1536) dtype &lt;f32&gt;\n        Torch time: 0.02398 ms\n        LLAISYS time: 0.00412 ms\n   idx_shape (1,) embd_shape (151936, 1536) dtype &lt;f16&gt;\n        Torch time: 0.02421 ms\n        LLAISYS time: 0.00399 ms\n   idx_shape (1,) embd_shape (151936, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.02420 ms\n        LLAISYS time: 0.00395 ms\n   idx_shape (128,) embd_shape (151936, 1536) dtype &lt;f32&gt;\n        Torch time: 0.02407 ms\n        LLAISYS time: 0.00413 ms\n   idx_shape (128,) embd_shape (151936, 1536) dtype &lt;f16&gt;\n        Torch time: 0.02398 ms\n        LLAISYS time: 0.00400 ms\n   idx_shape (128,) embd_shape (151936, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.02404 ms\n        LLAISYS time: 0.00406 ms\n   idx_shape (1,) embd_shape (151936, 4096) dtype &lt;f32&gt;\n        Torch time: 0.02393 ms\n        LLAISYS time: 0.00400 ms\n   idx_shape (1,) embd_shape (151936, 4096) dtype &lt;f16&gt;\n        Torch time: 0.02433 ms\n        LLAISYS time: 0.00407 ms\n   idx_shape (1,) embd_shape (151936, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.02427 ms\n        LLAISYS time: 0.00409 ms\n   idx_shape (512,) embd_shape (151936, 4096) dtype &lt;f32&gt;\n        Torch time: 0.02416 ms\n        LLAISYS time: 0.00666 ms\n   idx_shape (512,) embd_shape (151936, 4096) dtype &lt;f16&gt;\n        Torch time: 0.02432 ms\n        LLAISYS time: 0.00507 ms\n   idx_shape (512,) embd_shape (151936, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.02441 ms\n        LLAISYS time: 0.00505 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#linear","title":"Linear","text":"Bash<pre><code>Testing Ops.linear on nvidia\n   out (2, 3), x (2, 4), w (3, 4), bias True, dtype &lt;f32&gt;\n        Torch time: 0.01249 ms\n        LLAISYS time: 0.00431 ms\n   out (2, 3), x (2, 4), w (3, 4), bias True, dtype &lt;f16&gt;\n        Torch time: 0.01369 ms\n        LLAISYS time: 0.00799 ms\n   out (2, 3), x (2, 4), w (3, 4), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.01217 ms\n        LLAISYS time: 0.00815 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 0.51268 ms\n        LLAISYS time: 0.54491 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.14773 ms\n        LLAISYS time: 0.30438 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.12002 ms\n        LLAISYS time: 0.31846 ms\n   out (1, 1536), x (1, 1536), w (1536, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.01709 ms\n        LLAISYS time: 0.00741 ms\n   out (1, 1536), x (1, 1536), w (1536, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 0.01720 ms\n        LLAISYS time: 0.00746 ms\n   out (1, 1536), x (1, 1536), w (1536, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.01724 ms\n        LLAISYS time: 0.00752 ms\n   out (1, 8960), x (1, 1536), w (8960, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.01757 ms\n        LLAISYS time: 0.01412 ms\n   out (1, 8960), x (1, 1536), w (8960, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 0.01777 ms\n        LLAISYS time: 0.01048 ms\n   out (1, 8960), x (1, 1536), w (8960, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.01732 ms\n        LLAISYS time: 0.00967 ms\n   out (1, 1536), x (1, 8960), w (1536, 8960), bias True, dtype &lt;f32&gt;\n        Torch time: 0.06018 ms\n        LLAISYS time: 0.01462 ms\n   out (1, 1536), x (1, 8960), w (1536, 8960), bias True, dtype &lt;f16&gt;\n        Torch time: 0.02574 ms\n        LLAISYS time: 0.00857 ms\n   out (1, 1536), x (1, 8960), w (1536, 8960), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.02594 ms\n        LLAISYS time: 0.00845 ms\n   out (128, 1536), x (128, 1536), w (1536, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.02978 ms\n        LLAISYS time: 0.11133 ms\n   out (128, 1536), x (128, 1536), w (1536, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 0.02168 ms\n        LLAISYS time: 0.09791 ms\n   out (128, 1536), x (128, 1536), w (1536, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.02223 ms\n        LLAISYS time: 0.11025 ms\n   out (128, 8960), x (128, 1536), w (8960, 1536), bias True, dtype &lt;f32&gt;\n        Torch time: 0.11401 ms\n        LLAISYS time: 0.13132 ms\n   out (128, 8960), x (128, 1536), w (8960, 1536), bias True, dtype &lt;f16&gt;\n        Torch time: 0.03720 ms\n        LLAISYS time: 0.11676 ms\n   out (128, 8960), x (128, 1536), w (8960, 1536), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.03770 ms\n        LLAISYS time: 0.11960 ms\n   out (128, 1536), x (128, 8960), w (1536, 8960), bias True, dtype &lt;f32&gt;\n        Torch time: 0.14446 ms\n        LLAISYS time: 0.63794 ms\n   out (128, 1536), x (128, 8960), w (1536, 8960), bias True, dtype &lt;f16&gt;\n        Torch time: 0.03369 ms\n        LLAISYS time: 0.53992 ms\n   out (128, 1536), x (128, 8960), w (1536, 8960), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.05412 ms\n        LLAISYS time: 0.62192 ms\n   out (1, 4096), x (1, 4096), w (4096, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 0.01866 ms\n        LLAISYS time: 0.01591 ms\n   out (1, 4096), x (1, 4096), w (4096, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.01634 ms\n        LLAISYS time: 0.01011 ms\n   out (1, 4096), x (1, 4096), w (4096, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.01621 ms\n        LLAISYS time: 0.00927 ms\n   out (1, 12288), x (1, 4096), w (12288, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 0.21249 ms\n        LLAISYS time: 0.21264 ms\n   out (1, 12288), x (1, 4096), w (12288, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.10768 ms\n        LLAISYS time: 0.10740 ms\n   out (1, 12288), x (1, 4096), w (12288, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.10763 ms\n        LLAISYS time: 0.10741 ms\n   out (1, 4096), x (1, 12288), w (4096, 12288), bias True, dtype &lt;f32&gt;\n        Torch time: 0.24152 ms\n        LLAISYS time: 0.21360 ms\n   out (1, 4096), x (1, 12288), w (4096, 12288), bias True, dtype &lt;f16&gt;\n        Torch time: 0.11002 ms\n        LLAISYS time: 0.10753 ms\n   out (1, 4096), x (1, 12288), w (4096, 12288), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.10999 ms\n        LLAISYS time: 0.10748 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 0.50659 ms\n        LLAISYS time: 0.55326 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.15381 ms\n        LLAISYS time: 0.31083 ms\n   out (512, 4096), x (512, 4096), w (4096, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.11864 ms\n        LLAISYS time: 0.28828 ms\n   out (512, 12288), x (512, 4096), w (12288, 4096), bias True, dtype &lt;f32&gt;\n        Torch time: 1.47987 ms\n        LLAISYS time: 2.13214 ms\n   out (512, 12288), x (512, 4096), w (12288, 4096), bias True, dtype &lt;f16&gt;\n        Torch time: 0.38819 ms\n        LLAISYS time: 0.62012 ms\n   out (512, 12288), x (512, 4096), w (12288, 4096), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.37167 ms\n        LLAISYS time: 0.67751 ms\n   out (512, 4096), x (512, 12288), w (4096, 12288), bias True, dtype &lt;f32&gt;\n        Torch time: 1.28415 ms\n        LLAISYS time: 1.55384 ms\n   out (512, 4096), x (512, 12288), w (4096, 12288), bias True, dtype &lt;f16&gt;\n        Torch time: 0.38083 ms\n        LLAISYS time: 0.83333 ms\n   out (512, 4096), x (512, 12288), w (4096, 12288), bias True, dtype &lt;bf16&gt;\n        Torch time: 0.39655 ms\n        LLAISYS time: 0.96201 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#rms-norm","title":"RMS Norm","text":"Bash<pre><code>Testing Ops.rms_norm on nvidia\n   shape (1, 4) dtype &lt;f32&gt;\n        Torch time: 0.04497 ms\n        LLAISYS time: 0.00423 ms\n   shape (1, 4) dtype &lt;f16&gt;\n        Torch time: 0.04464 ms\n        LLAISYS time: 0.00420 ms\n   shape (1, 4) dtype &lt;bf16&gt;\n        Torch time: 0.04429 ms\n        LLAISYS time: 0.00426 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.04523 ms\n        LLAISYS time: 0.00884 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.04601 ms\n        LLAISYS time: 0.00726 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.04475 ms\n        LLAISYS time: 0.00724 ms\n   shape (1, 1536) dtype &lt;f32&gt;\n        Torch time: 0.04410 ms\n        LLAISYS time: 0.00432 ms\n   shape (1, 1536) dtype &lt;f16&gt;\n        Torch time: 0.04457 ms\n        LLAISYS time: 0.00428 ms\n   shape (1, 1536) dtype &lt;bf16&gt;\n        Torch time: 0.04404 ms\n        LLAISYS time: 0.00426 ms\n   shape (1, 4096) dtype &lt;f32&gt;\n        Torch time: 0.04449 ms\n        LLAISYS time: 0.00593 ms\n   shape (1, 4096) dtype &lt;f16&gt;\n        Torch time: 0.04408 ms\n        LLAISYS time: 0.00608 ms\n   shape (1, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.04402 ms\n        LLAISYS time: 0.00608 ms\n   shape (128, 4096) dtype &lt;f32&gt;\n        Torch time: 0.04526 ms\n        LLAISYS time: 0.00643 ms\n   shape (128, 4096) dtype &lt;f16&gt;\n        Torch time: 0.04507 ms\n        LLAISYS time: 0.00641 ms\n   shape (128, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.04457 ms\n        LLAISYS time: 0.00641 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.04492 ms\n        LLAISYS time: 0.00872 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.04487 ms\n        LLAISYS time: 0.00728 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.04456 ms\n        LLAISYS time: 0.00728 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#rope","title":"RoPE","text":"Bash<pre><code>Testing Ops.rope on nvidia\n   shape (2, 1, 4) range (0, 2) dtype &lt;f32&gt;\n        Torch time: 0.25043 ms\n        LLAISYS time: 0.00663 ms\n   shape (2, 1, 4) range (0, 2) dtype &lt;f16&gt;\n        Torch time: 0.24940 ms\n        LLAISYS time: 0.00669 ms\n   shape (2, 1, 4) range (0, 2) dtype &lt;bf16&gt;\n        Torch time: 0.24952 ms\n        LLAISYS time: 0.00667 ms\n   shape (512, 4, 4096) range (512, 1024) dtype &lt;f32&gt;\n        Torch time: 0.25730 ms\n        LLAISYS time: 0.03593 ms\n   shape (512, 4, 4096) range (512, 1024) dtype &lt;f16&gt;\n        Torch time: 0.25620 ms\n        LLAISYS time: 0.03622 ms\n   shape (512, 4, 4096) range (512, 1024) dtype &lt;bf16&gt;\n        Torch time: 0.25532 ms\n        LLAISYS time: 0.03624 ms\n   shape (1, 12, 128) range (128, 129) dtype &lt;f32&gt;\n        Torch time: 0.24449 ms\n        LLAISYS time: 0.00658 ms\n   shape (1, 12, 128) range (128, 129) dtype &lt;f16&gt;\n        Torch time: 0.24686 ms\n        LLAISYS time: 0.00674 ms\n   shape (1, 12, 128) range (128, 129) dtype &lt;bf16&gt;\n        Torch time: 0.24539 ms\n        LLAISYS time: 0.00669 ms\n   shape (1, 2, 128) range (128, 129) dtype &lt;f32&gt;\n        Torch time: 0.24487 ms\n        LLAISYS time: 0.00659 ms\n   shape (1, 2, 128) range (128, 129) dtype &lt;f16&gt;\n        Torch time: 0.23454 ms\n        LLAISYS time: 0.00460 ms\n   shape (1, 2, 128) range (128, 129) dtype &lt;bf16&gt;\n        Torch time: 0.25491 ms\n        LLAISYS time: 0.00623 ms\n   shape (128, 12, 128) range (0, 128) dtype &lt;f32&gt;\n        Torch time: 0.25739 ms\n        LLAISYS time: 0.00438 ms\n   shape (128, 12, 128) range (0, 128) dtype &lt;f16&gt;\n        Torch time: 0.24065 ms\n        LLAISYS time: 0.00581 ms\n   shape (128, 12, 128) range (0, 128) dtype &lt;bf16&gt;\n        Torch time: 0.18893 ms\n        LLAISYS time: 0.00437 ms\n   shape (128, 2, 128) range (0, 128) dtype &lt;f32&gt;\n        Torch time: 0.18709 ms\n        LLAISYS time: 0.00438 ms\n   shape (128, 2, 128) range (0, 128) dtype &lt;f16&gt;\n        Torch time: 0.18922 ms\n        LLAISYS time: 0.00443 ms\n   shape (128, 2, 128) range (0, 128) dtype &lt;bf16&gt;\n        Torch time: 0.20858 ms\n        LLAISYS time: 0.00527 ms\n   shape (512, 32, 128) range (0, 512) dtype &lt;f32&gt;\n        Torch time: 0.20561 ms\n        LLAISYS time: 0.01168 ms\n   shape (512, 32, 128) range (0, 512) dtype &lt;f16&gt;\n        Torch time: 0.21643 ms\n        LLAISYS time: 0.01143 ms\n   shape (512, 32, 128) range (0, 512) dtype &lt;bf16&gt;\n        Torch time: 0.20649 ms\n        LLAISYS time: 0.01147 ms\n   shape (512, 8, 128) range (0, 512) dtype &lt;f32&gt;\n        Torch time: 0.20329 ms\n        LLAISYS time: 0.00521 ms\n   shape (512, 8, 128) range (0, 512) dtype &lt;f16&gt;\n        Torch time: 0.20434 ms\n        LLAISYS time: 0.00543 ms\n   shape (512, 8, 128) range (0, 512) dtype &lt;bf16&gt;\n        Torch time: 0.20582 ms\n        LLAISYS time: 0.00530 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#self-attention","title":"Self-Attention","text":"Bash<pre><code>Testing Ops.self_attention on nvidia\n   qlen=2 kvlen=2 nh=1 nkvh=1 hd=4 dtype &lt;f32&gt;\n        Torch time: 0.19629 ms\n        LLAISYS time: 0.00453 ms\n   qlen=2 kvlen=2 nh=1 nkvh=1 hd=4 dtype &lt;f16&gt;\n        Torch time: 0.20629 ms\n        LLAISYS time: 0.00454 ms\n   qlen=2 kvlen=2 nh=1 nkvh=1 hd=4 dtype &lt;bf16&gt;\n        Torch time: 0.20379 ms\n        LLAISYS time: 0.00465 ms\n   qlen=5 kvlen=11 nh=4 nkvh=2 hd=8 dtype &lt;f32&gt;\n        Torch time: 0.19590 ms\n        LLAISYS time: 0.00454 ms\n   qlen=5 kvlen=11 nh=4 nkvh=2 hd=8 dtype &lt;f16&gt;\n        Torch time: 0.20131 ms\n        LLAISYS time: 0.00458 ms\n   qlen=5 kvlen=11 nh=4 nkvh=2 hd=8 dtype &lt;bf16&gt;\n        Torch time: 0.19876 ms\n        LLAISYS time: 0.00469 ms\n   qlen=128 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.19587 ms\n        LLAISYS time: 0.02705 ms\n   qlen=128 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f16&gt;\n        Torch time: 0.20215 ms\n        LLAISYS time: 0.02694 ms\n   qlen=128 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.20194 ms\n        LLAISYS time: 0.02693 ms\n   qlen=1 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.19998 ms\n        LLAISYS time: 0.01175 ms\n   qlen=1 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;f16&gt;\n        Torch time: 0.20260 ms\n        LLAISYS time: 0.01180 ms\n   qlen=1 kvlen=128 nh=12 nkvh=2 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.20149 ms\n        LLAISYS time: 0.01181 ms\n   qlen=512 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.27270 ms\n        LLAISYS time: 0.83442 ms\n   qlen=512 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f16&gt;\n        Torch time: 0.20366 ms\n        LLAISYS time: 0.75940 ms\n   qlen=512 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.21946 ms\n        LLAISYS time: 0.77392 ms\n   qlen=1 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f32&gt;\n        Torch time: 0.20096 ms\n        LLAISYS time: 0.04079 ms\n   qlen=1 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;f16&gt;\n        Torch time: 0.20212 ms\n        LLAISYS time: 0.03742 ms\n   qlen=1 kvlen=512 nh=32 nkvh=8 hd=128 dtype &lt;bf16&gt;\n        Torch time: 0.20180 ms\n        LLAISYS time: 0.03738 ms\nTest passed!\n</code></pre>"},{"location":"ops/nvidia/#swiglu","title":"SwiGLU","text":"Bash<pre><code>esting Ops.swiglu on nvidia\n   shape (2, 3) dtype &lt;f32&gt;\n        Torch time: 0.04492 ms\n        LLAISYS time: 0.00492 ms\n   shape (2, 3) dtype &lt;f16&gt;\n        Torch time: 0.06409 ms\n        LLAISYS time: 0.00485 ms\n   shape (2, 3) dtype &lt;bf16&gt;\n        Torch time: 0.06418 ms\n        LLAISYS time: 0.00490 ms\n   shape (512, 4096) dtype &lt;f32&gt;\n        Torch time: 0.04548 ms\n        LLAISYS time: 0.00758 ms\n   shape (512, 4096) dtype &lt;f16&gt;\n        Torch time: 0.06466 ms\n        LLAISYS time: 0.00647 ms\n   shape (512, 4096) dtype &lt;bf16&gt;\n        Torch time: 0.06474 ms\n        LLAISYS time: 0.00670 ms\n   shape (128, 8960) dtype &lt;f32&gt;\n        Torch time: 0.04515 ms\n        LLAISYS time: 0.00501 ms\n   shape (128, 8960) dtype &lt;f16&gt;\n        Torch time: 0.06448 ms\n        LLAISYS time: 0.00471 ms\n   shape (128, 8960) dtype &lt;bf16&gt;\n        Torch time: 0.06453 ms\n        LLAISYS time: 0.00474 ms\n   shape (1, 8960) dtype &lt;f32&gt;\n        Torch time: 0.04447 ms\n        LLAISYS time: 0.00473 ms\n   shape (1, 8960) dtype &lt;f16&gt;\n        Torch time: 0.06341 ms\n        LLAISYS time: 0.00473 ms\n   shape (1, 8960) dtype &lt;bf16&gt;\n        Torch time: 0.06389 ms\n        LLAISYS time: 0.00484 ms\n   shape (512, 12288) dtype &lt;f32&gt;\n        Torch time: 0.15665 ms\n        LLAISYS time: 0.03002 ms\n   shape (512, 12288) dtype &lt;f16&gt;\n        Torch time: 0.10051 ms\n        LLAISYS time: 0.01546 ms\n   shape (512, 12288) dtype &lt;bf16&gt;\n        Torch time: 0.10071 ms\n        LLAISYS time: 0.01631 ms\n   shape (1, 12288) dtype &lt;f32&gt;\n        Torch time: 0.04454 ms\n        LLAISYS time: 0.00480 ms\n   shape (1, 12288) dtype &lt;f16&gt;\n        Torch time: 0.06377 ms\n        LLAISYS time: 0.00478 ms\n   shape (1, 12288) dtype &lt;bf16&gt;\n        Torch time: 0.06414 ms\n        LLAISYS time: 0.00470 ms\nTest passed!\n</code></pre>"}]}